- config-keys:
    - gptoss-fp4-b200-vllm
    - gptoss-fp4-h100-vllm
    - gptoss-fp4-h200-vllm
  description: |
    - Upgrade vLLM from 0.10.2 to 0.11.0 for GPT-OSS NVIDIA single-node configs
    - Adds compilation-config: '{"cudagraph_mode":"PIECEWISE"} accordingly since vLLM 0.11.0
      requires now defaults to FULL_AND_PIECEWISE
    PR: https://github.com/InferenceMAX/InferenceMAX/pull/159
- config-keys:
    - dsr1-fp4-b200-sglang
    - dsr1-fp8-b200-sglang
    - dsr1-fp8-h200-sglang
  description: |
    - Consolidates H200 and B200 SGLang configurations to use unified v0.5.5-cu129-amd64 
      image tag and updates deprecated SGLang server arguments to their current equivalents.
    - --enable-flashinfer-trtllm-moe & --enable-ep-moe is no longer available in sglang so we needed to change it
      - ep: 4 for all tp: 4 entries (3 occurrences in dsr1-fp4-b200-sglang)
      - ep: 8 for all tp: 8 entries (6 occurrences across dsr1-fp4-b200-sglang and dsr1-fp8-b200-sglang)
    - dsr1_fp4_b200_docker.sh: Replaced --enable-ep-moe with --ep-size $EP_SIZE and --enable-flashinfer-trtllm-moe with 
      --moe-runner-backend flashinfer_trtllm
    - dsr1_fp8_b200_docker.sh: Replaced --enable-flashinfer-trtllm-moe with --moe-runner-backend flashinfer_trtllm and 
      added --ep-size $EP_SIZE
    - launch_b200-nvd.sh: Added -e EP_SIZE to Docker run command to pass environment variable to container
    - launch_b200-tg.sh: Added -e EP_SIZE to Docker run command to pass environment variable to container
    PR: https://github.com/InferenceMAX/InferenceMAX/pull/204
- config-keys:
    - gptoss-fp4-mi355x-vllm
    - gptoss-fp4-b200-vllm
  description: |
    - Extend concurrency to 128 for gptoss mi355x/b200 vllm configurations
    PR: https://github.com/InferenceMAX/InferenceMAX/pull/209
- config-keys:
    - gptoss-fp4-b200-trt
  description: |
    - Extend concurrency to 128 for gptoss b200 TRT configurations
    PR: https://github.com/InferenceMAX/InferenceMAX/pull/233
- config-keys:
    - dsr1-fp8-h200-trt
  description: |
    - Update TRT image from nvcr.io#nvidia/tensorrt-llm/release:1.2.0rc0.post1 to nvcr.io#nvidia/tensorrt-llm/release:1.2.0rc2
    - Increase concurrency for some configurations
    PR: https://github.com/InferenceMAX/InferenceMAX/pull/266
- config-keys:
    - gptoss-fp4-b200-vllm
    - gptoss-fp4-h100-vllm
    - gptoss-fp4-h200-vllm
  description: |
    - Update vLLM image for NVIDIA configs from vLLM 0.11.0 to vLLM 0.11.2
    - Adds kv-cache-dtype: fp8 to benchmarks/gptoss_fp4_b200_docker.sh
    PR: https://github.com/InferenceMAX/InferenceMAX/pull/273



